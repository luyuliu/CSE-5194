qsub -I -l nodes=4:ppn=28 -l walltime=2:00:00 -A PAS1588

qsub -I -l nodes=2:ppn=1:gpus=1 -l walltime=2:00:00 -A PAS1588

module load python/3.6-conda5.2 
cd /users/PAS1588/liuluyu0378/example/transformers/examples
cd CSE-5194/labs/lab2

export OMP_NUM_THREADS=28



    --train_data_file /users/PAS1588/liuluyu0378/example/datas/glue_data/MNLI/train.tsv \
    --train_data_file /users/PAS1588/liuluyu0378/example/datas/train.tsv \
    --train_data_file /users/PAS1588/liuluyu0378/example/datas/glue_data/CoLA/original/tokenized/in_domain_train.tsv \

python run_lm_finetuning.py \
    --mlm \
    --train_data_file /users/PAS1588/liuluyu0378/example/datas/glue_data/MRPC/train.tsv \
    --output_dir /users/PAS1588/liuluyu0378/example/datas/ \
    --overwrite_output_dir \
    --do_train \
    --num_train_epochs 2

python run_lm_finetuning.py \
    --mlm \
    --train_data_file /users/PAS1588/liuluyu0378/lab1/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00001-of-00100 \
    --output_dir /users/PAS1588/liuluyu0378/example/datas/ \
    --overwrite_output_dir \
    --do_train \
    --num_train_epochs 1 \
    --per_gpu_train_batch_size 4 \
    --local_rank 0 \
    --world_size 2


    --evaluate_during_training \
    --do_eval \
    --eval_data_file /users/PAS1588/liuluyu0378/lab1/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00002-of-00100 \
    
python run_lm_finetuning.py \
    --mlm \
    --train_data_file /users/PAS1588/liuluyu0378/example/datas/glue_data/CoLA/original/tokenized/in_domain_train.tsv \
    --output_dir /users/PAS1588/liuluyu0378/example/datas/ \
    --overwrite_output_dir \
    --do_train \
    --num_train_epochs 2

python run_lm_finetuning.py --mlm --train_data_file /users/PAS1588/liuluyu0378/example/datas/glue_data/CoLA/original/tokenized/in_domain_train.tsv --output_dir /users/PAS1588/liuluyu0378/example/datas/ --overwrite_output_dir --do_train --num_train_epochs 2

python run_lm_finetuning.py --mlm --train_data_file /users/PAS1588/liuluyu0378/lab1/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00001-of-00100 --output_dir /users/PAS1588/liuluyu0378/example/datas/ --overwrite_output_dir --do_train --do_eval --eval_data_file /users/PAS1588/liuluyu0378/lab1/1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled/news.en-00002-of-00100 --num_train_epochs 2 --evaluate_during_training


source $PATH_TO_MINICONDA/bin/activate
conda create -n pytorch_horovod python=2.7.15 (creating conda environment. you can use python 3.6.5 also).
export PYTHONNOUSERSITE=true
conda activate pytorch_horovod 
pip install torch torchvision

module load cuda/10.0.130
export HOROVOD_CUDA_HOME=/usr/local/cuda/10.0.130
export HOROVOD_CUDA_HOME=/apps/cuda/10.0.130/
HOROVOD_GPU_ALLREDUCE=MPI pip install --no-cache-dir  horovod==0.16.4

